{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTO7xPd07FdK",
    "outputId": "822effda-d2a5-4784-df27-a0a00cecf95c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Science Fundamental Steps\n",
      "PAGES BY PERSON:\n",
      "\n",
      "    Data Collection - Cierra\n",
      "    Data Cleaning - Raven\n",
      "    Data Exploration - Bousaina\n",
      "    Data Preparation - Joycelyn\n",
      "    Model Training - Abdul-Malik\n",
      "    Model Evaluation - Wendesen\n",
      "    Model Deployment - Ajeune\n",
      "    \n",
      "______\n",
      "\n",
      "SOME COMMENTED CODE REPRESENTS CODE FOR STREAMLIT ONLY - THIS CODE WAS WRITEN SOLELY IN STREAMLIT VIA .PY FILE AND IS BEING\n",
      " BROUGHT OVER TO THIS FILE FOR REFERENCE.\n",
      "______\n"
     ]
    }
   ],
   "source": [
    "#Main Page\n",
    "print(\"Data Science Fundamental Steps\")\n",
    "print(\"PAGES BY PERSON:\")\n",
    "pages = \"\"\"\n",
    "    Data Collection - Cierra\n",
    "    Data Cleaning - Raven\n",
    "    Data Exploration - Bousaina\n",
    "    Data Preparation - Joycelyn\n",
    "    Model Training - Abdul-Malik\n",
    "    Model Evaluation - Wendesen\n",
    "    Model Deployment - Ajeune\n",
    "    \"\"\"\n",
    "print(pages)\n",
    "print(\"______\\n\")\n",
    "print(\"SOME COMMENTED CODE REPRESENTS CODE FOR STREAMLIT ONLY - THIS CODE WAS WRITEN SOLELY IN STREAMLIT VIA .PY FILE AND IS BEING\\n BROUGHT OVER TO THIS FILE FOR REFERENCE.\")\n",
    "print(\"______\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL STEPS IMPORT STATEMENTS - this cell includes all the necessary import statements for the remaining cells\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "rPQyIOBS523t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Collection\n",
      "Data collection is the process of gathering or bringing together data (various pieces of information) relevant to your specific research goals or targets.  Data can be collected from a variety of methods and sources such as information from datasets or databases, web scraped data, live measurements, surveys/interviews, observations, experiments, and/or countless other means.\n",
      "\n",
      "Data is collected with the intention of solving a problem or gathering insights for a particular subject, therefore, it is important to collect comprehensive and relevant data.  Collect data from reputable sources and ensure completeness prior to the next steps in the data science process because proper data collection directly affects the accuracy and reliability of the overall data analysis.\n",
      "\n",
      "Be sure to only collect and use data with proper permissions and be cognizant of any policies and laws that may affect you during the data collection phase.\n",
      "\n",
      "Click the button below to view 2018 HBCU data.\n",
      "\n",
      "HBCU Data\n",
      "First five rows of the HBCU 2018 dataset:\n",
      "                       Name        City State  acceptance_rate  Enrollment  \\\n",
      "0    Alabama A&M University  Huntsville    AL            52.56      3930.0   \n",
      "1  Alabama State University  Montgomery    AL            53.26      4383.0   \n",
      "2   Albany State University      Albany    GA            47.29      2780.0   \n",
      "3   Alcorn State University     Fayette    MS            78.44      2570.0   \n",
      "4          Allen University    Columbia    SC           100.00       642.0   \n",
      "\n",
      "      Type        Lat       Long  \n",
      "0   Public  34.784633 -86.572031  \n",
      "1   Public  32.363662 -86.294054  \n",
      "2   Public  31.569032 -84.140424  \n",
      "3   Public  31.874204 -91.137292  \n",
      "4  Private  34.010694 -81.020246  \n",
      "\n",
      "\n",
      "Last five rows of the HBCU 2018 dataset:\n",
      "                              Name           City State  acceptance_rate  \\\n",
      "91  West Virginia State University         Dunbar    WV            41.44   \n",
      "92          Wilberforce University    Wilberforce    OH            37.54   \n",
      "93                   Wiley College       Marshall    TX           100.00   \n",
      "94  Winston-Salem State University  Winston-Salem    NC            60.16   \n",
      "95  Xavier University of Louisiana    New Orleans    LA            65.99   \n",
      "\n",
      "    Enrollment     Type        Lat       Long  \n",
      "91      1950.0   Public  38.378959 -81.766588  \n",
      "92       348.0  Private  39.710638 -83.878929  \n",
      "93      1259.0  Private  32.538487 -94.375187  \n",
      "94      4098.0   Public  36.089874 -80.225117  \n",
      "95      2237.0  Private  29.963863 -90.107084  \n"
     ]
    }
   ],
   "source": [
    "#Page 1- Data Collection\n",
    "\n",
    "\n",
    "#Title\n",
    "print(\"Data Collection\")\n",
    "\n",
    "\n",
    "#data collection definition\n",
    "data_collection_text = \"\"\"Data collection is the process of gathering or bringing together data (various pieces of information) relevant to your specific research goals or targets.  Data can be collected from a variety of methods and sources such as information from datasets or databases, web scraped data, live measurements, surveys/interviews, observations, experiments, and/or countless other means.\n",
    "\n",
    "Data is collected with the intention of solving a problem or gathering insights for a particular subject, therefore, it is important to collect comprehensive and relevant data.  Collect data from reputable sources and ensure completeness prior to the next steps in the data science process because proper data collection directly affects the accuracy and reliability of the overall data analysis.\n",
    "\n",
    "Be sure to only collect and use data with proper permissions and be cognizant of any policies and laws that may affect you during the data collection phase.\n",
    "\n",
    "Click the button below to view 2018 HBCU data.\n",
    "\"\"\"\n",
    "\n",
    "print(data_collection_text)\n",
    "\n",
    "#importing the data\n",
    "hbcu_data = pd.read_csv('hbcus_2018.csv')\n",
    "\n",
    "print(\"HBCU Data\")\n",
    "#displaying first 5 rows of the dataset\n",
    "print(\"First five rows of the HBCU 2018 dataset:\")\n",
    "print(hbcu_data.head())\n",
    "print(\"\\n\")\n",
    "#displaying last 5 rows of the dataset\n",
    "print(\"Last five rows of the HBCU 2018 dataset:\")\n",
    "print(hbcu_data.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWt5sC0c6Ao0",
    "outputId": "add23b73-adf8-40e6-eab0-80a0e3d486e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning\n",
      "Data cleaning involves pre-processing the data to ensure that it’s accurate, complete, and consistent. This includes tasks such as removing duplicates, filling in missing values, and standardizing the format.\n",
      "First 10 entries HBCU Dataset:\n",
      "                         Name           City State  acceptance_rate  \\\n",
      "0      Alabama A&M University     Huntsville    AL            52.56   \n",
      "1    Alabama State University     Montgomery    AL            53.26   \n",
      "2     Albany State University         Albany    GA            47.29   \n",
      "3     Alcorn State University        Fayette    MS            78.44   \n",
      "4            Allen University       Columbia    SC           100.00   \n",
      "5    American Baptist College      Nashville    TN           100.00   \n",
      "6    Arkansas Baptist College    Little Rock    AR           100.00   \n",
      "7            Benedict College       Columbia    SC           100.00   \n",
      "8   Bennett College for Women     Greensboro    NC            91.79   \n",
      "9  Bethune-Cookman University  Daytona Beach    FL            63.71   \n",
      "\n",
      "   Enrollment     Type        Lat       Long  \n",
      "0      3930.0   Public  34.784633 -86.572031  \n",
      "1      4383.0   Public  32.363662 -86.294054  \n",
      "2      2780.0   Public  31.569032 -84.140424  \n",
      "3      2570.0   Public  31.874204 -91.137292  \n",
      "4       642.0  Private  34.010694 -81.020246  \n",
      "5       110.0  Private  36.202408 -86.789867  \n",
      "6       796.0  Private  34.735060 -92.289542  \n",
      "7      2405.0  Private  34.012867 -81.020461  \n",
      "8       545.0  Private  36.067872 -79.779216  \n",
      "9      3695.0  Private  29.211010 -81.031575  \n",
      "Using the hbcu_2018 dataset, let's take a look at the importance of data cleaning step.\n",
      "\n",
      "Inconsistent Column Names\n",
      "The current data has column format discrepancies. Some names feature different cases while other introduce symbols.\n",
      "Current Column Names\n",
      "{'Enrollment', 'Type', 'State', 'Name', 'City', 'Lat', 'Long', 'acceptance_rate'}\n",
      "Column name discrepancies: Different cases (i.e., lowercase, uppercase); Spacing vs. underscores\n",
      "Standardized*** Column Names\n",
      "{'lat', 'state', 'enrollment', 'city', 'long', 'type', 'name', 'acceptance_rate'}\n",
      "The column names are now uniform in case. For any columns with 2+ words, the underline syntax will remain.\n",
      "\n",
      "Column Datatypes\n",
      "name                object\n",
      "city                object\n",
      "state               object\n",
      "acceptance_rate    float64\n",
      "enrollment         float64\n",
      "type                object\n",
      "lat                float64\n",
      "long               float64\n",
      "dtype: object\n",
      "0    3930.0\n",
      "1    4383.0\n",
      "2    2780.0\n",
      "3    2570.0\n",
      "4     642.0\n",
      "Name: enrollment, dtype: float64\n",
      "\n",
      "Currently, the enrollment column is of type float64. However, when it comes to student enrollment, a **fraction** of a person cannot be enrolled.Therefore, conversion to int64 datatype, a whole number representation, would be more feasible.\n",
      "Fill in the NaN values with 0 for the enrollment column (reason: prep work to convert dtype to int from float\n",
      " PREP CODE (reference) >>>hbcus['enrollment'] = hbcus['enrollment'].fillna(0)\n",
      "After conversion to int datatype:\n",
      "name                object\n",
      "city                object\n",
      "state               object\n",
      "acceptance_rate    float64\n",
      "enrollment           int64\n",
      "type                object\n",
      "lat                float64\n",
      "long               float64\n",
      "dtype: object\n",
      "name               0\n",
      "city               0\n",
      "state              0\n",
      "acceptance_rate    0\n",
      "enrollment         0\n",
      "type               0\n",
      "lat                0\n",
      "long               0\n",
      "dtype: int64\n",
      "\n",
      "The above are a few examples of how data cleaning can be tangibly be seen. The data cleaning step can be one of the longest steps to truly accomplish, depending on the size of the dataset, discrepancies in the data, and much more.\n"
     ]
    }
   ],
   "source": [
    "#Page 2- Data Cleaning\n",
    "print(\"Data Cleaning\")\n",
    "data_cleaning_1 = \"Data cleaning involves pre-processing the data to ensure that it’s accurate, complete, and consistent. This includes tasks such as removing duplicates, filling in missing values, and standardizing the format.\"\n",
    "\n",
    "\n",
    "# Display introductory text about data cleaning\n",
    "print(data_cleaning_1)\n",
    "\n",
    "\n",
    "# Create dataframe from CSV file; hbcus dataframe creation\n",
    "hbcus = pd.read_csv(\"hbcus_2018.csv\")\n",
    "\n",
    "# HBCU Data toggle button\n",
    "# Display the first 10 entries in a dataframe\n",
    "print(\"First 10 entries HBCU Dataset:\")\n",
    "print(hbcus.head(10))\n",
    "\n",
    "print(\"Using the hbcu_2018 dataset, let's take a look at the importance of data cleaning step.\\n\")\n",
    "print(\"Inconsistent Column Names\")\n",
    "print(\"The current data has column format discrepancies. Some names feature different cases while other introduce symbols.\")\n",
    "\n",
    "\n",
    "print(\"Current Column Names\")\n",
    "og_cols = set(hbcus.columns)\n",
    "print(og_cols)\n",
    "icn = \"Column name discrepancies: Different cases (i.e., lowercase, uppercase); Spacing vs. underscores\"\n",
    "print(icn)\n",
    "\n",
    "# print(icn)\n",
    "\n",
    "print(\"Standardized*** Column Names\")\n",
    "hbcus.columns = hbcus.columns.str.lower()\n",
    "new_cols = set(hbcus.columns)\n",
    "print(new_cols)\n",
    "print(\"The column names are now uniform in case. For any columns with 2+ words, the underline syntax will remain.\")\n",
    "# CODE REPRESENTS CODE FOR STREAMLIT ONLY - CODE WAS WRITEN SOLELY IN STREAMLIT VIA .PY FILE\\n\")\n",
    "# st.link_button(\":blue[Learn More...]\", \"https://towardsdatascience.com/how-to-clean-messy-pandas-column-names-20dc7400cea7\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nColumn Datatypes\")\n",
    "print(hbcus.dtypes)\n",
    "enr_col = hbcus[\"enrollment\"]\n",
    "print(enr_col.head(5))\n",
    "print(\"\"\"\\nCurrently, the enrollment column is of type float64. However, when it comes to student enrollment, a **fraction** of a person cannot be enrolled.Therefore, conversion to int64 datatype, a whole number representation, would be more feasible.\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Change the columns names to all lowercase  (reason: unifying the format of the column names; some were uppercase, some were lowercase)\n",
    "hbcus.columns = hbcus.columns.str.lower()\n",
    "\n",
    "print(\"Fill in the NaN values with 0 for the enrollment column (reason: prep work to convert dtype to int from float\")\n",
    "print(\" PREP CODE (reference) >>>hbcus['enrollment'] = hbcus['enrollment'].fillna(0)\")\n",
    "\n",
    "\n",
    "# Replace NaN values with an empty string value \"\"\n",
    "hbcus['acceptance_rate'] = hbcus['acceptance_rate'].fillna(0)\n",
    "\n",
    "\n",
    "# Change enrollment dtype from float to int (reason: original data was in int form; there cannot be a fraction of a person enrolled)\n",
    "hbcus[\"enrollment\"] = hbcus[\"enrollment\"].astype(int)\n",
    "print(\"After conversion to int datatype:\")\n",
    "print(hbcus.dtypes)\n",
    "\n",
    "\n",
    "\n",
    "# Result of Cleaning Data - Null Values Accounted For\n",
    "print(hbcus.isna().sum())\n",
    "\n",
    "\n",
    "print(\"\\nThe above are a few examples of how data cleaning can be tangibly be seen. The data cleaning step can be one of the longest steps to truly accomplish, depending on the size of the dataset, discrepancies in the data, and much more.\")\n",
    "\n",
    "#COMMENTED CODE REPRESENTS CODE FOR STREAMLIT ONLY - CODE WAS WRITEN SOLELY IN STREAMLIT VIA .PY FILE\n",
    "#st.link_button(\":blue[Learn more...]\", \"https://medium.com/@abhishiktadhar111/a-comprehensive-guide-to-data-cleaning-techniques-ebb2659c89a7\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nzx5zkv_6PBG",
    "outputId": "ecddca28-c892-48f0-c933-761f92bb129b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Exploration\n",
      "Data exploration is the first step of data analysis used to explore and visualize data to uncover insights from the start and identify areas or patterns to dig into further. This is when the story the data scientist is seeking to tell is told. This is the time to dig deeper into the data and find any themes, patterns, outliers, and disparities. Through this level of analysis, users can extract the bigger picture from the data and use it to draw insights.\n",
      "\n",
      "Data exploration is a critical step in any data analysis project. It helps us understand the dataset, identify patterns, and gain insights. Regardless of one's profession within data science, data exploration sets the stage for the story you will tell through data, and what problems or solutions you will draw from your analysis. Data visualization tools, such as creating bar graphs, pie charts, or maps, are instrumental tools to communicating initial findings in data analysis. Below are several examples of data exploration, including searching for relationship betweeen variables, outliers, and looking for patterns and relationships using data visualization tools.\n",
      "\n",
      "**First Five Rows of the Data**\n",
      "                       Name        City State  acceptance_rate  Enrollment  \\\n",
      "0    Alabama A&M University  Huntsville    AL            52.56      3930.0   \n",
      "1  Alabama State University  Montgomery    AL            53.26      4383.0   \n",
      "2   Albany State University      Albany    GA            47.29      2780.0   \n",
      "3   Alcorn State University     Fayette    MS            78.44      2570.0   \n",
      "4          Allen University    Columbia    SC           100.00       642.0   \n",
      "\n",
      "      Type        Lat       Long  \n",
      "0   Public  34.784633 -86.572031  \n",
      "1   Public  32.363662 -86.294054  \n",
      "2   Public  31.569032 -84.140424  \n",
      "3   Public  31.874204 -91.137292  \n",
      "4  Private  34.010694 -81.020246  \n",
      "**Summary Statistics**\n",
      "       acceptance_rate   Enrollment        Lat       Long\n",
      "count        96.000000    96.000000  96.000000  96.000000\n",
      "mean         68.645417  2144.552083  34.413842 -85.267883\n",
      "std          27.094794  1767.591438   2.905410   6.206974\n",
      "min          16.330000   110.000000  25.919008 -98.454820\n",
      "25%          46.455000   886.250000  32.531219 -90.120495\n",
      "50%          63.980000  1682.500000  34.011781 -85.736058\n",
      "75%         100.000000  2758.250000  36.166920 -80.450257\n",
      "max         100.000000  8423.000000  39.933506 -75.525429\n",
      "**Distribution of Numerical Columns**\n",
      "\n",
      "COMMENTED CODE REPRESENTS CODE FOR STREAMLIT ONLY - CODE WAS WRITEN SOLELY IN STREAMLIT VIA .PY FILE\n",
      "\n",
      "**Relationships Between Variables**\n",
      "**Geospatial Exploration**\n",
      "**Public vs. Private Bar Graph**\n",
      "**Public vs. Private Pie Chart**\n",
      "**Heat Map of Enrollment**\n",
      "\n",
      "COMMENTED CODE REPRESENTS CODE FOR STREAMLIT ONLY - CODE WAS WRITEN SOLELY IN STREAMLIT VIA .PY FILE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Page 3- Data Exploration\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "print(\"Data Exploration\")\n",
    "\n",
    "print(\"\"\"Data exploration is the first step of data analysis used to explore and visualize data to uncover insights from the start and identify areas or patterns to dig into further. This is when the story the data scientist is seeking to tell is told. This is the time to dig deeper into the data and find any themes, patterns, outliers, and disparities. Through this level of analysis, users can extract the bigger picture from the data and use it to draw insights.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"Data exploration is a critical step in any data analysis project. It helps us understand the dataset, identify patterns, and gain insights. Regardless of one's profession within data science, data exploration sets the stage for the story you will tell through data, and what problems or solutions you will draw from your analysis. Data visualization tools, such as creating bar graphs, pie charts, or maps, are instrumental tools to communicating initial findings in data analysis. Below are several examples of data exploration, including searching for relationship betweeen variables, outliers, and looking for patterns and relationships using data visualization tools.\n",
    "\"\"\")\n",
    "\n",
    "hbcu = pd.read_csv(\"hbcus_2018.csv\")\n",
    "\n",
    "print(\"**First Five Rows of the Data**\")\n",
    "print(hbcu.head())\n",
    "\n",
    "\n",
    "print(\"**Summary Statistics**\")\n",
    "print(hbcu.describe())\n",
    "\n",
    "\n",
    "print(\"**Distribution of Numerical Columns**\")\n",
    "numerical_columns = hbcu.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "\n",
    "print(\"\\nCOMMENTED CODE REPRESENTS CODE FOR STREAMLIT ONLY - CODE WAS WRITEN SOLELY IN STREAMLIT VIA .PY FILE\\n\")\n",
    "# for column in numerical_columns:\n",
    "#     st.write(f\"### {column}\")\n",
    "#     st.bar_chart(hbcu[column])\n",
    "\n",
    "\n",
    "print(\"**Relationships Between Variables**\")\n",
    "# sns.pairplot(hbcu)\n",
    "# pairplot_fig = plt.gcf()  # Get the current figure\n",
    "#st.pyplot(pairplot_fig)\n",
    "\n",
    "\n",
    "print(\"**Geospatial Exploration**\")\n",
    "hbcu.rename(columns={'Lat': 'latitude', 'Long': 'longitude'}, inplace=True)\n",
    "#st.map(hbcu)\n",
    "\n",
    "\n",
    "print(\"**Public vs. Private Bar Graph**\")\n",
    "hbcu_enrollment = hbcu.groupby('Type')['Enrollment'].sum()\n",
    "hbcu_mean_enrollment = hbcu.groupby('Type')['Enrollment'].mean()\n",
    "# fig, ax = plt.subplots()\n",
    "# hbcu_mean_enrollment.plot(kind='bar', color=['red', 'blue'], ax=ax)\n",
    "# ax.set_ylabel('HBCU Student Enrollment')\n",
    "# ax.set_title('HBCU Enrollment: Public vs. Private')\n",
    "#st.pyplot(fig)\n",
    "\n",
    "\n",
    "print(\"**Public vs. Private Pie Chart**\")\n",
    "hbcu_enrollment = hbcu.groupby('Type')['Enrollment'].sum()\n",
    "# fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# ax.pie(hbcu_enrollment, labels=hbcu_enrollment.index, autopct='%1.1f%%', colors=['red', 'blue'])\n",
    "# ax.set_title('Distribution of HBCU Enrollment: Public vs. Private')\n",
    "#st.pyplot(fig)\n",
    "\n",
    "\n",
    "print(\"**Heat Map of Enrollment**\")\n",
    "hbcu['Enrollment'] = pd.to_numeric(hbcu['Enrollment'], errors='coerce')\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# #sns.heatmap(hbcu.pivot_table(index='latitude', columns='longitude', values='Enrollment', aggfunc=np.mean), cmap='viridis', annot=True, fmt=\".0f\")\n",
    "# heatmap_fig = plt.gcf()  # Get the current figure\n",
    "# #st.pyplot(heatmap_fig)\n",
    "\n",
    "print(\"\\nCOMMENTED CODE REPRESENTS CODE FOR STREAMLIT ONLY - CODE WAS WRITEN SOLELY IN STREAMLIT VIA .PY FILE\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JVQgq57p6cH4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Data Preparation***\n",
      "\n",
      "Data preparation is the process of gathering, cleaning, and organizing data for analysis.\n",
      "Feature engineering, normalization, and dimensionality reduction are components of data\n",
      "preparation. The goal of data preparation is to ensure that the data is accurate and\n",
      "consistent so that the results of applications are valid.\n",
      "\n",
      "\n",
      "***Cleaning the Data***\n",
      "\n",
      "To produce complete and accurate data sets, it is necessary to identify data errors\n",
      "and issues. Cleaning data sets involves removing or correcting faulty data, filling in\n",
      "missing values, and harmonizing inconsistent entries.\n",
      "\n",
      "***Exploration Data***\n",
      "By exploring data, one can fully comprehend what a dataset represents. Data exploration\n",
      "can help clarify what you are hoping to learn from the data and what questions you are hoping\n",
      "to answer. The data can be used to identify trends, patterns, inconsistencies, incompleteness,\n",
      "and relationships between the variables. Data exploration also helps to identify outliers,\n",
      "correlations, and outliers. It can also help to uncover any potential biases or errors in\n",
      "the data. Finally, it can provide insights into the data that can be used to make predictions\n",
      "or decisions.\n",
      "\n",
      "***Model Training***\n",
      "The preparation process may involve modeling the data and organizing it to meet analytics\n",
      "requirements. It is possible, for instance, to convert CSV files into tables so they can be\n",
      "accessed by analytics tools.\n",
      "\n",
      "***Model Evaluation***\n",
      "Model evaluation is the process of analyzing a model's performance and its strengths and\n",
      "weaknesses using different evaluation metrics. Evaluation plays a vital role in assessing\n",
      "the efficacy of a model during its initial research phase, as well as in monitoring the model.\n",
      "\n",
      "***Model Deployment***\n",
      "A final step in the validation process involves running automated routines against the data to\n",
      "ensure its consistency, completeness, and accuracy. The prepared data is then stored and used\n",
      "directly by the preparer who can make it available for other users to access.\n"
     ]
    }
   ],
   "source": [
    "#Page 4- Data Preparation\n",
    "\n",
    "print(\"***Data Preparation***\")\n",
    "print(\"\"\"\n",
    "Data preparation is the process of gathering, cleaning, and organizing data for analysis.\n",
    "Feature engineering, normalization, and dimensionality reduction are components of data\n",
    "preparation. The goal of data preparation is to ensure that the data is accurate and\n",
    "consistent so that the results of applications are valid.\\n\n",
    "\"\"\")\n",
    "print(\"***Cleaning the Data***\")\n",
    "\n",
    "print(\"\"\"\n",
    "To produce complete and accurate data sets, it is necessary to identify data errors\n",
    "and issues. Cleaning data sets involves removing or correcting faulty data, filling in\n",
    "missing values, and harmonizing inconsistent entries.\\n\"\"\")\n",
    "\n",
    "print(\"***Exploration Data***\")\n",
    "print(\"\"\"By exploring data, one can fully comprehend what a dataset represents. Data exploration\n",
    "can help clarify what you are hoping to learn from the data and what questions you are hoping\n",
    "to answer. The data can be used to identify trends, patterns, inconsistencies, incompleteness,\n",
    "and relationships between the variables. Data exploration also helps to identify outliers,\n",
    "correlations, and outliers. It can also help to uncover any potential biases or errors in\n",
    "the data. Finally, it can provide insights into the data that can be used to make predictions\n",
    "or decisions.\\n\"\"\")\n",
    "\n",
    "print(\"***Model Training***\")\n",
    "print(\"\"\"The preparation process may involve modeling the data and organizing it to meet analytics\n",
    "requirements. It is possible, for instance, to convert CSV files into tables so they can be\n",
    "accessed by analytics tools.\\n\"\"\")\n",
    "\n",
    "print(\"***Model Evaluation***\")\n",
    "print(\"\"\"Model evaluation is the process of analyzing a model's performance and its strengths and\n",
    "weaknesses using different evaluation metrics. Evaluation plays a vital role in assessing\n",
    "the efficacy of a model during its initial research phase, as well as in monitoring the model.\\n\"\"\")\n",
    "\n",
    "print(\"***Model Deployment***\")\n",
    "print(\"\"\"A final step in the validation process involves running automated routines against the data to\n",
    "ensure its consistency, completeness, and accuracy. The prepared data is then stored and used\n",
    "directly by the preparer who can make it available for other users to access.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "3ZLOg0vV6kaX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training\n",
      "\n",
      "Based on our previous data analysis processes we present four models for enrollment prediction and analysis of historically Black colleges and universities (HBCUs) in this Model Training part. The outlined four models investigate the relationship between acceptance rates and enrollment levels among the top 20 universities with highest and lowest enrollment figures. Findings suggest acceptance rates alone do not fully explain enrollment trends, emphasizing the need to consider additional factors like academic reputation and location. The model's insights underscore the complexity of enrollment dynamics in HBCUs.\n",
      "\n",
      "HBCUs Enrollment Prediction/Analysis.\n",
      "Model 1: Enrollment Prediction for Top 20 Universities with Highest Enrollment\n",
      "\n",
      "**Model 1 Summary:**\n",
      "This model focuses on predicting the enrollment of the top 20 universities with the highest enrollment based on their acceptance rates. It uses linear regression to establish a relationship between acceptance rates and enrollment, providing insights into how changes in acceptance rates may impact enrollment in these universities. The summary statistics generated from the OLS regression provide information about the intercept, coefficient, and goodness of fit (R-squared) of the regression model.\n",
      "\n",
      "Summary of Statistics\n",
      "\n",
      "**Model 1 Conclusion and Predictions:**\n",
      "The R-squared value of 0.000 and adjusted R-squared value of -0.071 for Model 1 suggest that the acceptance rate alone does not explain enrollment variability in universities with high enrollment figures. This indicates that factors beyond acceptance rate, such as academic reputation and campus facilities, may play a more significant role in enrollment decisions. Therefore, universities aiming to maintain or increase enrollment should focus on enhancing various aspects of their offerings rather than solely relying on changes in acceptance rates.\n",
      "\n",
      "Model 2: Enrollment Prediction for Top 20 Universities with Lowest Enrollment\n",
      "\n",
      "**Model 2 Summary:**\n",
      "Similar to Model 1, this model aims to predict enrollment but focuses on the top 20 universities with the lowest enrollment. It also utilizes linear regression to establish a relationship between acceptance rates and enrollment but for universities with lower enrollment figures. The summary statistics from the OLS regression offer insights into the intercept, coefficient, and goodness of fit of the regression model for this subset of universities.\n",
      "\n",
      "Summary of Statistics\n",
      "\n",
      "**Model 2 Conclusion and Predictions:**\n",
      "The R-squared value of 0.006 for Model 2 indicates that the acceptance rate alone explains only a small fraction of the variability in enrollment. Moreover, the negative adjusted R-squared value of -0.065 suggests that the model's explanatory power diminishes when considering additional predictors and sample size. These findings imply that the acceptance rate may not be a robust predictor of enrollment levels for universities with the lowest enrollment figures. Other factors like academic reputation, location, program offerings, and financial aid availability likely play a more significant role. Therefore, universities should adopt a holistic approach to enrollment management, considering various aspects to attract and retain students effectively.\n",
      "\n",
      "Model 3: Comparison between Model 1 and Model 2\n",
      "\n",
      "**Model 3 Summary:**\n",
      "Model 3 compares the enrollment characteristics between the top 20 universities with the highest enrollment (Model 1) and the top 20 universities with the lowest enrollment (Model 2). It calculates and presents the mean enrollment values for both groups of universities and examines the correlation between their acceptance rates. This comparison provides insights into any disparities or similarities in enrollment patterns between universities with different enrollment levels.\n",
      "\n",
      "Model 3 Values\n",
      "   Mean Enrollment Model 1  Mean Enrollment Model 2  \\\n",
      "0                   5053.7                    443.6   \n",
      "\n",
      "   Correlation between acceptance_rate of Model 1 and Model 2  \n",
      "0                                               -0.1           \n",
      "\n",
      "**Model 3 Conclusion and Predictions:**\n",
      "The negative correlation of -0.1 between the acceptance rates of Model 1 and Model 2 suggests a subtle yet noteworthy relationship between acceptance rates and enrollment levels. It implies that universities with lower acceptance rates may attract more applicants, leading to higher enrollment figures. This phenomenon underscores the interplay between competitiveness and enrollment, highlighting the significance of market positioning and enrollment strategies for universities. While maintaining high academic standards is essential, ensuring accessibility and diversity within the student body is crucial for long-term success. Policymakers and education stakeholders should consider the implications of enrollment trends on resource allocation and policy development to address the evolving needs of higher education institutions.\n",
      "\n",
      "Model 4: Proximity Analysis\n",
      "\n",
      "**Model 4 Summary:**\n",
      "This model conducts a proximity analysis to predict the possibility of nearby schools having a split of enrollment. It utilizes geographical coordinates (latitude and longitude) of universities to calculate distances between them. The scatter plot visualizes the geographical distribution of universities and their enrollment figures, allowing for the identification of clusters or patterns in enrollment. This analysis helps understand whether the proximity of universities influences their enrollment dynamics, potentially leading to insights about competition or collaboration among nearby institutions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Page 5- Model Training\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import matplotlib.pyplot as plt\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "print(\"Model Training\")\n",
    "open_text = \"\"\"\n",
    "Based on our previous data analysis processes we present four models for enrollment prediction and analysis of historically Black colleges and universities (HBCUs) in this Model Training part. The outlined four models investigate the relationship between acceptance rates and enrollment levels among the top 20 universities with highest and lowest enrollment figures. Findings suggest acceptance rates alone do not fully explain enrollment trends, emphasizing the need to consider additional factors like academic reputation and location. The model's insights underscore the complexity of enrollment dynamics in HBCUs.\n",
    "\"\"\"\n",
    "\n",
    "print(open_text)\n",
    "\n",
    "print(\"HBCUs Enrollment Prediction/Analysis.\")\n",
    "\n",
    "\n",
    "data =  pd.read_csv(\"hbcus_2018.csv\")\n",
    "\n",
    "top_20_high_enrollment = data.nlargest(20, 'Enrollment')\n",
    "\n",
    "print(\"Model 1: Enrollment Prediction for Top 20 Universities with Highest Enrollment\")\n",
    "mod_1_sum = \"\"\"\n",
    "**Model 1 Summary:**\n",
    "This model focuses on predicting the enrollment of the top 20 universities with the highest enrollment based on their acceptance rates. It uses linear regression to establish a relationship between acceptance rates and enrollment, providing insights into how changes in acceptance rates may impact enrollment in these universities. The summary statistics generated from the OLS regression provide information about the intercept, coefficient, and goodness of fit (R-squared) of the regression model.\n",
    "\"\"\"\n",
    "print(mod_1_sum)\n",
    "\n",
    "X1 = top_20_high_enrollment[['acceptance_rate']]\n",
    "y1 = top_20_high_enrollment['Enrollment']\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X1_train, y1_train)\n",
    "y1_pred = model1.predict(X1_test)\n",
    "\n",
    "X1_train = sm.add_constant(X1_train)\n",
    "model1_ols = sm.OLS(y1_train, X1_train).fit()\n",
    "\n",
    "print(\"Summary of Statistics\")\n",
    "# CODE REPRESENTS CODE FOR STREAMLIT ONLY - CODE WAS WRITEN SOLELY IN STREAMLIT VIA .PY FILE\\n\")\n",
    "# st.code(model1_ols.summary(), language='html')\n",
    "mod_1_con = \"\"\"\n",
    "**Model 1 Conclusion and Predictions:**\n",
    "The R-squared value of 0.000 and adjusted R-squared value of -0.071 for Model 1 suggest that the acceptance rate alone does not explain enrollment variability in universities with high enrollment figures. This indicates that factors beyond acceptance rate, such as academic reputation and campus facilities, may play a more significant role in enrollment decisions. Therefore, universities aiming to maintain or increase enrollment should focus on enhancing various aspects of their offerings rather than solely relying on changes in acceptance rates.\n",
    "\"\"\"\n",
    "\n",
    "print(mod_1_con)\n",
    "\n",
    "print(\"Model 2: Enrollment Prediction for Top 20 Universities with Lowest Enrollment\")\n",
    "mod_2_sum = \"\"\"\n",
    "**Model 2 Summary:**\n",
    "Similar to Model 1, this model aims to predict enrollment but focuses on the top 20 universities with the lowest enrollment. It also utilizes linear regression to establish a relationship between acceptance rates and enrollment but for universities with lower enrollment figures. The summary statistics from the OLS regression offer insights into the intercept, coefficient, and goodness of fit of the regression model for this subset of universities.\n",
    "\"\"\"\n",
    "\n",
    "print(mod_2_sum)\n",
    "\n",
    "top_20_low_enrollment = data.nsmallest(20, 'Enrollment')\n",
    "X2 = top_20_low_enrollment[['acceptance_rate']]\n",
    "y2 = top_20_low_enrollment['Enrollment']\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X2_train, y2_train)\n",
    "y2_pred = model2.predict(X2_test)\n",
    "\n",
    "X2_train = sm.add_constant(X2_train)\n",
    "model2_ols = sm.OLS(y2_train, X2_train).fit()\n",
    "\n",
    "print(\"Summary of Statistics\")\n",
    "# CODE REPRESENTS CODE FOR STREAMLIT ONLY - CODE WAS WRITEN SOLELY IN STREAMLIT VIA .PY FILE\\n\")\n",
    "#st.code(model2_ols.summary(), language='html')\n",
    "mod_2_con = \"\"\"\n",
    "**Model 2 Conclusion and Predictions:**\n",
    "The R-squared value of 0.006 for Model 2 indicates that the acceptance rate alone explains only a small fraction of the variability in enrollment. Moreover, the negative adjusted R-squared value of -0.065 suggests that the model's explanatory power diminishes when considering additional predictors and sample size. These findings imply that the acceptance rate may not be a robust predictor of enrollment levels for universities with the lowest enrollment figures. Other factors like academic reputation, location, program offerings, and financial aid availability likely play a more significant role. Therefore, universities should adopt a holistic approach to enrollment management, considering various aspects to attract and retain students effectively.\n",
    "\"\"\"\n",
    "\n",
    "print(mod_2_con)\n",
    "\n",
    "print(\"Model 3: Comparison between Model 1 and Model 2\")\n",
    "mod_3_sum = \"\"\"\n",
    "**Model 3 Summary:**\n",
    "Model 3 compares the enrollment characteristics between the top 20 universities with the highest enrollment (Model 1) and the top 20 universities with the lowest enrollment (Model 2). It calculates and presents the mean enrollment values for both groups of universities and examines the correlation between their acceptance rates. This comparison provides insights into any disparities or similarities in enrollment patterns between universities with different enrollment levels.\n",
    "\"\"\"\n",
    "\n",
    "print(mod_3_sum)\n",
    "\n",
    "mean_enrollment_model1 = round(top_20_high_enrollment['Enrollment'].mean(), 1)\n",
    "mean_enrollment_model2 = round(top_20_low_enrollment['Enrollment'].mean(), 1)\n",
    "correlation = round(np.corrcoef(top_20_high_enrollment['acceptance_rate'], top_20_low_enrollment['acceptance_rate'])[0, 1], 1)\n",
    "\n",
    "model3_df = pd.DataFrame({\n",
    "  \"Mean Enrollment Model 1\": [mean_enrollment_model1],\n",
    "  \"Mean Enrollment Model 2\": [mean_enrollment_model2],\n",
    "  \"Correlation between acceptance_rate of Model 1 and Model 2\": [correlation]\n",
    "})\n",
    "\n",
    "print(\"Model 3 Values\")\n",
    "print(model3_df)\n",
    "mod_3_conp = \"\"\"\n",
    "**Model 3 Conclusion and Predictions:**\n",
    "The negative correlation of -0.1 between the acceptance rates of Model 1 and Model 2 suggests a subtle yet noteworthy relationship between acceptance rates and enrollment levels. It implies that universities with lower acceptance rates may attract more applicants, leading to higher enrollment figures. This phenomenon underscores the interplay between competitiveness and enrollment, highlighting the significance of market positioning and enrollment strategies for universities. While maintaining high academic standards is essential, ensuring accessibility and diversity within the student body is crucial for long-term success. Policymakers and education stakeholders should consider the implications of enrollment trends on resource allocation and policy development to address the evolving needs of higher education institutions.\n",
    "\"\"\"\n",
    "\n",
    "print(mod_3_conp)\n",
    "\n",
    "print(\"Model 4: Proximity Analysis\")\n",
    "mod_4_sum = \"\"\"\n",
    "**Model 4 Summary:**\n",
    "This model conducts a proximity analysis to predict the possibility of nearby schools having a split of enrollment. It utilizes geographical coordinates (latitude and longitude) of universities to calculate distances between them. The scatter plot visualizes the geographical distribution of universities and their enrollment figures, allowing for the identification of clusters or patterns in enrollment. This analysis helps understand whether the proximity of universities influences their enrollment dynamics, potentially leading to insights about competition or collaboration among nearby institutions.\n",
    "\"\"\"\n",
    "\n",
    "print(mod_4_sum)\n",
    "\n",
    "# CODE REPRESENTS CODE FOR STREAMLIT ONLY - CODE WAS WRITEN SOLELY IN STREAMLIT VIA .PY FILE\")\n",
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# scatter = ax.scatter(data['Long'], data['Lat'], c=data['Enrollment'], cmap='viridis')\n",
    "# plt.title(\"Geographical Distribution of Universities and Enrollment\")\n",
    "# plt.xlabel(\"Longitude\")\n",
    "# plt.ylabel(\"Latitude\")\n",
    "# plt.colorbar(scatter, label=\"Enrollment\")\n",
    "# pyplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8vdRTYCj6t30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation\n",
      "\n",
      "insert text here\n"
     ]
    }
   ],
   "source": [
    "#Page 6- Model Evaluation\n",
    "print(\"Model Evaluation\\n\")\n",
    "print(\"insert text here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3cAyVglw60S_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Deployment\n",
      "\n",
      "The deployment of this application will allow the user to analyze relationships between historically Black colleges and universities and their respective enrollment rates. The application can be widely used by a number of stakeholders. Firstly, the admissions offices and higher education professionals can use the application to inform policy and to ensure that decisions are made with accurate data and objectivity. As data analysis continues to become a regular aspect of the academic landscape, this application would allow seamless data integration into systems used each day like Handshake and Blackboard.\n",
      "\n",
      "\n",
      "In addition, the application's deployment as a web-based application will allow prospective students to access enrollment information via computer or mobile device. This application can be accessed by parents and high school administrators, who assist the students with making enrollment decisions. The application can also be shared with third-party partners of HBCUs who provide funding and other opportunities. Government agencies, both state-wide and federal can utilize the application to have up to date information on the state of HBCUs. Technical deployment of the application can be shared via Streamlit for mobile and internet use. The applications model features geospatial data which can pinpoint the location of HBCUs for an interactive map within the application. Cloud-based web deployment be would more widely accessible and can allow this team of web developers to consistently update the data provided in real time.\n"
     ]
    }
   ],
   "source": [
    "#Page 7- Model Deployment\n",
    "print(\"Model Deployment\\n\")\n",
    "\n",
    "para1 = (\"\"\"The deployment of this application will allow the user to analyze relationships between historically Black colleges and universities and their respective enrollment rates. The application can be widely used by a number of stakeholders. Firstly, the admissions offices and higher education professionals can use the application to inform policy and to ensure that decisions are made with accurate data and objectivity. As data analysis continues to become a regular aspect of the academic landscape, this application would allow seamless data integration into systems used each day like Handshake and Blackboard.\"\"\")\n",
    "para2 = (\"\"\"In addition, the application's deployment as a web-based application will allow prospective students to access enrollment information via computer or mobile device. This application can be accessed by parents and high school administrators, who assist the students with making enrollment decisions. The application can also be shared with third-party partners of HBCUs who provide funding and other opportunities. Government agencies, both state-wide and federal can utilize the application to have up to date information on the state of HBCUs. Technical deployment of the application can be shared via Streamlit for mobile and internet use. The applications model features geospatial data which can pinpoint the location of HBCUs for an interactive map within the application. Cloud-based web deployment be would more widely accessible and can allow this team of web developers to consistently update the data provided in real time.\"\"\")\n",
    "\n",
    "print(para1)\n",
    "print(\"\\n\")\n",
    "print(para2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
